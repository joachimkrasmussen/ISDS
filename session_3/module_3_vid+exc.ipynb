{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Videos and Exercises for Session 3: Data Structuring in Pandas II\n",
    "\n",
    "In this combined teaching module and exercise set, you will continue working with structuring data. \n",
    "\n",
    "In the last session, you were working with making operations on relatively clean data. However, before it is meaningful to make such operations, you will (as a data scientist) often have to do some very preliminary cleaning, involving for instance dealing with missings and duplicates as well as combining and restructuring larger sets of data.\n",
    "\n",
    "In this session, you will learn how to do such data processing. This notebook is structured as follows:\n",
    "1. Missings and Duplicated Data:\n",
    "    - Handling Missings: Delete or Interpolate?\n",
    "    - Spotting and Interpreting Duplicates\n",
    "2. Combining Data Sets:\n",
    "    - Intro to `merge`, `concat` and `join`\n",
    "    - Horizontal and Vertical Merging\n",
    "3. Split-Apply-Combine\n",
    "    - Finding Means and Other Characteristics from Data Subsets (aggregation)\n",
    "4. Reshaping Data\n",
    "    - Wide and Long Data\n",
    "    - Intro to `stack`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Packages\n",
    "Before we get started.... load in the required modules and set up the plotting library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Part 1: Duplicates and Missings\n",
    "\n",
    "In this section we will use [this dataset](https://archive.ics.uci.edu/ml/datasets/Adult) from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html) to practice some basic operations on pandas dataframes. This is an extract from the US Census containing almost 50,000 rwos of individual-level micro data from 1994.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.1.1:** This link `'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'` leads to a comma-separated file with income data from a US census. Load the data into a pandas dataframe and show the 25th to 35th row.\n",
    "\n",
    "> _Hint 1:_ There are no column names in the dataset. Use the list `['age','workclass', 'fnlwgt', 'educ', 'educ_num', 'marital_status', 'occupation','relationship', 'race', 'sex','capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'wage']` as names. \n",
    "\n",
    "> _Hint 2:_ When you read in the csv, you might find that pandas includes whitespace in all of the cells. To get around this include the argument `skipinitialspace = True` to `read_csv()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "url = f'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "\n",
    "labels = ['age','workclass', 'fnlwgt', 'educ', 'educ_num', 'marital_status', 'occupation','relationship', 'race', 'sex','capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'wage']\n",
    "\n",
    "df_census = pd.read_csv(url, header=None, skipinitialspace = True, names=labels)\n",
    "df_census.iloc[24:35]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Missing data\n",
    "\n",
    "Often our data having information missing, e.g. one row lacks data on education for a specific person. Watch the video below about missing data type and get some simple tools to deal with the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('BLmXVUS8Ju0', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.1.2:** What is the missing value sign in this dataset? Replace all missing values with NA's understood by pandas. Then proceed to drop all rows containing any missing values with the `dropna` method. Store this new dataframe as `df_census_new`. How many rows are removed in this operation?\n",
    "\n",
    "> _Hint 1:_ if this doesn't work as expected you might want to take a look at the hint for 4.2.1 again.\n",
    " \n",
    "> _Hint 2:_ The NaN method from NumPy might be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import NaN\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "df_census_new = df_census.replace('?', NaN).dropna()\n",
    "print(f\"We have dropped {len(df_census) - len(df_census_new)} rows\")\n",
    "print(f\"This amounts to {round((len(df_census) - len(df_census_new))/len(df_census)*100,2)} % of the rows\")\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Duplicated Data\n",
    "\n",
    "Watch the video below about duplicated data - in particular duplicated rows and how to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('kjdd_TQSRME', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Video 2: Duplicated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.1.3:** Determine whether or not duplicated rows is a problem in the NOAA weather data and the US census data in the module. You should come up with arguments from the structure of the rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "print(f\"The share of duplicate observations in the weather data is {round((len(load_weather(1863)) - len(load_weather(1863).drop_duplicates(['station','datetime'])))/len(load_weather(1863))*100,2)} %\")\n",
    "print(f\"The share of duplicate observations in the US census data is {round((len(df_census_new) - len(df_census_new.drop_duplicates()))/len(df_census_new)*100,2)} %\")\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Combining Data Sets\n",
    "\n",
    "Below we hear about how different datasets can be combined into one, by merging on overlapping information that exists in both datasets. If you want to know more then you can look up Chapter 8, section 8.2 in Python for Data Analysis, 2ed.\n",
    "\n",
    "**Note:** In the video, we are only dealing with one-to-one joins. This is a type of merge where there is only one row with a given merge key in each data frame. However, you may encounter situations where there are multiple rows that share the value of a merge key. In this situation, you may perform a one-to-many join or a many-to-many join that forms a Cartesian product of your rows. You can read more about these types of merges by visiting the Jake van der Plass [link](https://jakevdp.github.io/PythonDataScienceHandbook/03.07-merge-and-join.html) or looking into PDA, section 8.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('ueP5K7zhUDU', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with the final part of three exercises on structuring weather data. Use the function for fetching and structuring weather data that you used in part 2 of this exercise (i.e. exercise section 4.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.2.1:** Get the processed data from years 1870-1875 as a list of DataFrames. Generate a variable that denotes the year that the observations belong to. Convert the list into a single DataFrame by concatenating vertically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "my_list = []\n",
    "for year in range(1870,1876):\n",
    "    df = load_weather(year)\n",
    "    df['year'] = year\n",
    "    my_list.append(df)\n",
    "\n",
    "df_weather = pd.concat(my_list, axis=0)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Split-Apply-Combine\n",
    "\n",
    "Often we need to process information for a given individual, point in time etc. Instead of writing a loop over all the subsets of the data, we can use a more clever approach. Below we introduce the split-apply-combine framework and show how we can leverage it in pandas. If you want to know more then you can look up Chapter 10 in Python for Data Analysis, 2ed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('PVBzPwR5YT8', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* The Split-Apply-Combine method uses `.groupby()`. As indicated in the video, `.groupby()` is a method of pandas dataframes, meaning we can call it like so: `data.groupby('colname')`. The method groups your dataset by a specified column, and applies any following changes within each of these groups. For a more detailed explanation see [this link](https://www.tutorialspoint.com/python_pandas/python_pandas_groupby.htm). The [documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html) might also be useful.\n",
    "\n",
    "> **Ex. 3.3.1:** Use the *split-apply-combine*-method to compute the mean maximum daily temperature for each month-year pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "split_var=['year', 'month']\n",
    "apply_var='obs_value'\n",
    "df_weather.groupby(split_var)[apply_var].mean()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.3.2:** Plot the monthly max,min, mean, first and third quartiles for maximum temperature for our station with the ID _'ITE00100550'_ for the years 1870-1875. \n",
    "\n",
    "> *Hint*: the method `describe` computes all these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "split_var = ['year', 'month']\n",
    "apply_var = 'obs_value'\n",
    "types = ['max', 'min', 'mean', '25%', '75%']\n",
    "df_weather[df_weather['station']=='ITE00100550'].groupby(split_var)[apply_var].describe()[types].plot(figsize=(11,6))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.3.3:** Parse the station location data which you can find at https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt. Merge station locations onto the weather data spanning 1870-1875.  \n",
    "\n",
    "> _Hint:_ The location data have the folllowing format, \n",
    "\n",
    "```\n",
    "------------------------------\n",
    "Variable   Columns   Type\n",
    "------------------------------\n",
    "ID            1-11   Character\n",
    "LATITUDE     13-20   Real\n",
    "LONGITUDE    22-30   Real\n",
    "ELEVATION    32-37   Real\n",
    "STATE        39-40   Character\n",
    "NAME         42-71   Character\n",
    "GSN FLAG     73-75   Character\n",
    "HCN/CRN FLAG 77-79   Character\n",
    "WMO ID       81-85   Character\n",
    "------------------------------\n",
    "```\n",
    "\n",
    "> *Hint*: The station information has *fixed width format* - does there exist a pandas reader for that? Here Google might be helpful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "url = f'https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n",
    "labels = ['station', 'latitude', 'longitude', 'elevation', 'state', 'name', 'gsn flag', 'hcn/crn flag', 'wmo id']\n",
    "\n",
    "df_location = pd.read_fwf(url, header=None, names = labels)\n",
    "\n",
    "df_weather_new = pd.merge(df_weather, df_location, on='station', how='left')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bonus Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to practice *split-apply-combine* a bit more before proceeding, we have generated two additional exercises that use the US census data from Part 1. In order to solve this exercise, you should use `df_census_new` which you generated in the last exercise.\n",
    "\n",
    "> **Ex. 3.3.4:** (_Bonus_) Is there any evidence of a gender-wage-gap in the data? Create a table showing the percentage of men and women earning more than 50K a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "df_census_new['HighWage'] = (df_census_new['wage'] == '>50K').astype(int)\n",
    "df_census_new[['sex', 'HighWage']].groupby('sex').mean()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.3.5:** (_Bonus_) Group the data by years of education (`educ_num`) and marital status. Now plot the share of individuals who earn more than 50K for the two groups 'Divorced' and 'Married-civ-spouse' (normal marriage). Your final result should look like this: \n",
    "\n",
    "![](examplefig.png)\n",
    "\n",
    "> _Hint:_ remember the `.query()` method is extremely useful for filtering data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "df_census_new[['marital_status', 'HighWage', 'educ_num']]\\\n",
    "        .groupby(['marital_status', 'educ_num'])\\\n",
    "        .mean()\\\n",
    "        .reset_index()\\\n",
    "        .query(\"marital_status == 'Divorced' | marital_status == 'Married-civ-spouse'\")\\\n",
    "        .set_index('educ_num')\\\n",
    "        .groupby('marital_status')\\\n",
    "        .HighWage\\\n",
    "        .plot()\n",
    "\n",
    "plt.xlabel('Years of education')\n",
    "plt.ylabel('Share earning more than 50K')\n",
    "plt.legend()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Reshaping  Dataframes\n",
    "\n",
    "Often we have data that comes in a format that does not fit our purpose. If you want to know more then you can look up Chapter 8, section 8.3 in Python for Data Analysis, 2ed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('6eANX7bA5Xs', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we continue working with the NOAA data that you worked with in the main section of part 3.\n",
    "\n",
    "> **Ex. 4.4.1:** For which months was the temperature in general colder in 1870 vs. 1875?\n",
    "\n",
    "> *Hint: you may use `unstack`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "split_var = ['year', 'month']\n",
    "apply_var = 'obs_value'\n",
    "types = ['mean']\n",
    "\n",
    "yearly_data = df_weather.groupby(split_var)[apply_var].describe()[types].unstack()\n",
    "\n",
    "for month in range(0,12):\n",
    "    if yearly_data.iloc[0,month]>yearly_data.iloc[5,month]:\n",
    "        print(f\"In month {month+1}, it was warmer in 1870 than in 1875\")\n",
    "    else:\n",
    "        print(f\"In month {month+1}, it was warmer in 1875 than in 1870\")\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Bonus Exercises $-$ Traffic Data in Copenhagen\n",
    "\n",
    "In this second part set of exercises you will be working with traffic data from Copenhagen Municipality.\n",
    "\n",
    "The municipality have made the data openly available through the [opendata.dk](http://www.opendata.dk/) platform. We will use the data from traffic counters to construct a dataset of hourly traffic. We will use this data to get basic insights on the development in traffic over time and relate it to weather. The gist here is to practice a very important skill in Data Science: being able to quickly fetch data from the web and structure it so that you can work with it. Scraping usually gets a bit more advanced than what we will do today, but the following exercises should give you a taste for how it works. The bulk of these exercise, however, revolve around using the Pandas library to structure and analyze data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.5.A: Getting some data to work with\n",
    "\n",
    "Hence follows a simple scraping exercise where you (1) collect urls for datasets in the webpage listing data on traffic counters and (2) use these urls to load the data into one dataframe.\n",
    "\n",
    "> **Ex. 3.5.1:** *(Bonus)* Using the requests module, extract the html markup of the webpage data.kk.dk/dataset/faste-trafiktaellinger and store it as a string in a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from requests import get \n",
    "\n",
    "url = 'https://www.opendata.dk/city-of-copenhagen/faste-trafiktaellinger'\n",
    "resp = get(url)\n",
    "\n",
    "html = resp.text\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.5.2:** *(Bonus)* Using the re module, extract a list of all the urls in the html string and store them in a new variable.\n",
    "\n",
    "> _Hint:_ Try using the re.findall method. You may want to Google around to figure out how to do this. Searching for something along the lines of \"extract all links in html regex python\" and hitting the first StackOverflow link will probably get you farther than reading elaborate documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "### BEGIN SOLUTION\n",
    "rehits = re.findall(r'href=[\\'\"]?([^\\'\" >]+)', html)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.5.3:** *(Bonus)* Create a new variable `datalinks` that only contains the links that point to downloadable traffic data sheets. Some links may be preset more than once on the page. To get the unique links use the `set()` function on `datalinks`.\n",
    "\n",
    "> _Hint:_ You want to filter the results from above. For example to only include urls with the term 'download' in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "datalinks = set([url for url in rehits if 'download' in url])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.5.4:** *(Bonus)* Using pd.read_excel method, load the datasets into a list. Your resulting variable should hold a list of Pandas dataframes.\n",
    "\n",
    "> _Hint:_ you may want to set the `skiprows` keyword argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "traffic_raw = [pd.read_excel(url, skiprows = 10) for url in datalinks]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.5.5:** *(Bonus)* Merge the list of dataframes into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "final_data = pd.concat(traffic_raw)\n",
    "final_data\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.5.B Structuring your data\n",
    "\n",
    "If you successfully completed the previous part, you should now have a dataframe with about 183.397 rows (if your number of rows is close but not the same, worry not—it matters little in the following). Well done! But the data is still in no shape for analysis, so we must clean it up a little.\n",
    "\n",
    "161.236 rows (and 30 columns) is a lot of data. ~3.3 MB by my back-of-the-envelope calculations, so not \"Big Data\", but still enough to make your CPU heat up if you don't use it carefully. Pandas is built to handle fairly large dataframes and has advanced functionality to perform very fast operations even when the size of your data grows huge. So instead of working with basic Python we recommend working pandas built-in procedures as they are constructed to be fast on dataframes.\n",
    "\n",
    "Nerd fact: the reason pandas is much faster than pure Python is that dataframes access a lower level programming languages (namely C, C++) which are multiple times faster than Python. The reason it is faster is that it has a higher level of explicitness and thus is more difficult to learn and navigate.\n",
    "\n",
    "> **Ex. 3.5.6:** *(Bonus)* Reset the row indices of your dataframe so the first index is 0 and the last is whatever the number of rows your dataframe has. Also drop the column named 'index' and the one named `Spor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "final_data = final_data\\\n",
    "        .reset_index()\\\n",
    "        .drop(['index', 'Spor'], axis = 1)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.5.7:** *(Bonus)* Rename variables from Danish to English using the dictionary below.\n",
    "\n",
    "```python \n",
    "dk_to_uk = {\n",
    "    'Vejnavn':'road_name',\n",
    "    '(UTM32)':'UTM32_north',\n",
    "    '(UTM32).1':'UTM32_east',\n",
    "    'Dato':'date',\n",
    "    'Vej-Id':'road_id'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "dk_to_uk = {\n",
    "    'Vejnavn':'road_name',\n",
    "    '(UTM32)':'UTM32_north',\n",
    "    '(UTM32).1':'UTM32_east',\n",
    "    'Dato':'date',\n",
    "    'Vej-Id':'road_id'\n",
    "}\n",
    "\n",
    "\n",
    "final_data = final_data.rename(columns = dk_to_uk)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is quite efficient. For example, when you create a new dataframe by manipulating an old one, Python notices that—apart from some minor changes—these two objects are almost the same. Since memory is a precious resource, Python will represent the values in the new dataframe as references to the variables in the old dataset. This is great for performance, but if you for whatever reason change some of the values in your old dataframe, values in the new one will also change—and we don't want that! Luckily, we can break this dependency.\n",
    "\n",
    "> **Ex. 3.5.8:** *(Bonus)* Break the dependencies of the dataframe that resulted from Ex. 5.2.7 using the `.copy` method. Delete all other dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "new_data = final_data.copy()\n",
    "\n",
    "del final_data\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have structured appropriately, something that you will want to do again and again is selecting subsets of the data. Specifically, it means that you select specific rows in the dataset based on some column values.\n",
    "\n",
    ">**Ex. 3.5.9:** *(Bonus)* Create a new column in the dataframe called total that is True when the last letter of road_id is T and otherwise False.\n",
    "\n",
    "> _Hint:_ you will need the `pd.Series.str` attribute for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "new_data['total'] = (new_data.road_id.str[-1] == 'T')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.5.10:** *(Bonus)* Select rows where total is True. Delete all the remaining observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "total_data = new_data[new_data.total == True]\n",
    "\n",
    "del new_data\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.5.11:** *(Bonus)* Make two datasets based on the lists of columns below. Call the dataset with spatial columns data_geo and the other data.\n",
    "\n",
    "```python\n",
    "# Columns for `geo_data`, stored in `geo_columns`\n",
    "spatial_columns = ['road_name', 'UTM32_north', 'UTM32_east']\n",
    "\n",
    "# Columns for `data`, stored in `select_columns`\n",
    "hours = ['kl.{}-{}'.format(str(h).zfill(2), str(h+1).zfill(2)) for h in range(24)]\n",
    "select_columns = ['road_name', 'date'] + hours\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Columns for `geo_data`, stored in `geo_columns`\n",
    "spatial_columns = ['road_name', 'UTM32_north', 'UTM32_east']\n",
    "\n",
    "# Columns for `data`, stored in `select_columns`\n",
    "hours = [f'kl.{str(h).zfill(2)}-{str(h+1).zfill(2)}' for h in range(24)]\n",
    "select_columns = ['road_name', 'date'] + hours\n",
    "\n",
    "# selections\n",
    "data_geo = total_data[spatial_columns]\n",
    "data  = total_data[select_columns]\n",
    "\n",
    "del total_data\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.5.12:** *(Bonus)* Drop the duplicate rows in data_geo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "data_geo.drop_duplicates(inplace = True)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formatting: wide and narrow format**\n",
    "\n",
    "When talking about two-dimensional data (matrices, tables or dataframes, we can call it many things), we can either say that it is in wide or long format (see explanation here, \"wide\" and \"long\" are used interchangably). In Pandas we can use the commands stack and unstack to move between these formats.\n",
    "\n",
    "The wide format has the advantage that it often requires less storage and is easier to read when printed. On the other hand the long format can be easier for modelling, because each observation has its own row. Turns out that the latter is what we most often need.\n",
    "\n",
    "> **Ex. 3.5.13:** *(Bonus)* Turn the dataset from wide to long so hourly data is now vertically stacked. Store this dataset in a dataframe called data. Name the column with hourly information hour_period. Your resulting dataframe should look something like this.\n",
    "\n",
    "> _Hint:_ pandas' melt function may be of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "# A bonus info: This function is written using optional type declaration. This\n",
    "# is used to communicate what type the inputs should be. It doesn't affect the \n",
    "# functionality in any way, but it can sometimes increase readability. \n",
    "\n",
    "def gather(df: pd.DataFrame, key: str, value: str, cols: list) -> pd.DataFrame:\n",
    "    \"\"\" This helper makes pythons melt() behave like R's gather()\n",
    "    \"\"\"\n",
    "    id_vars = [ col for col in df.columns if col not in cols ]\n",
    "    id_values = cols\n",
    "    var_name = key\n",
    "    value_name = value\n",
    "    return pd.melt( df, id_vars, id_values, var_name, value_name )\n",
    "\n",
    "data_long = gather(data, 'hour_period', 'value', hours)\n",
    "\n",
    "# # using .stack() \n",
    "# # dropna = False in stack() is necessary for the dataframe to be exactly equal to the one above\n",
    "# data_long = data.set_index([\"road_name\", \"date\"])\\\n",
    "#                 .stack(dropna=False)\\\n",
    "#                 .reset_index()\\\n",
    "#                 .rename(columns = {\"level_2\": \"hour_period\", 0: \"value\"})\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical data**\n",
    "\n",
    "Categorical data can contain Python objects, usually strings. These are smart if you have variables with string observations that are long and often repeated, e.g. with road names.\n",
    "\n",
    "> **Ex. 3.5.14:** *(Bonus)* Use the `.astype` method to convert the type of the road_name column to categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "data_long.road_name = data_long.road_name.astype('category')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure temporal data\n",
    "\n",
    "Pandas has native support for working with temporal data. This is handy as much 'big data' often has time stamps which we can make Pandas aware of. Once we have encoded temporal data it can be used to extract information such as the hour, second etc.\n",
    "\n",
    "> **Ex. 3.5.15:** *(Bonus)* Create a new column called hour which contains the hour-of-day for each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "data_long['hour'] = pd.to_datetime(data_long['hour_period'].str[3:5], format = '%H')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 3.5.16:** *(Bonus)* Create a new column called time, that contains the time of the row in datetime format. Delete the old temporal columns (hour, hour_period, date) to save memory.\n",
    "\n",
    "> _Hint:_ try making an intermediary series of strings that has all temporal information for the row; then use pandas to_datetime function where you can specify the format of the date string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "data_long['timestamp'] = pd.to_datetime(data_long.date + ' ' + data_long.hour\\\n",
    "                                                                        .dt\\\n",
    "                                                                        .hour\\\n",
    "                                                                        .astype(str)\\\n",
    "                                                                        .apply(lambda x: x.zfill(2)),\n",
    "                                        format = '%d.%m.%Y %H')\n",
    "\n",
    "data_long = data_long.drop(['hour', 'hour_period', 'date'], axis = 1)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.5.17:** *(Bonus)* Using your time column make a new column called weekday which stores the weekday (in values between 0 and 6) of the corresponding datetime.\n",
    "\n",
    "> _Hint:_ try using the dt method for the series called time; dt has some relevant methods itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "data_long['weekday'] = data_long.timestamp.dt.weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical descriptions of traffic data\n",
    "\n",
    "> **Ex. 3.5.18:** *(Bonus)* Print the \"descriptive statistics\" of the traffic column. Also show a kernel density estimate of the values.\n",
    "\n",
    "> _Hint:_ Use the describe method of pandas dataframes for the first task. Use seaborn for the second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "print(data_long.value.describe())\n",
    "\n",
    "sb.kdeplot(data_long.value.dropna())\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.5.19:** *(Bonus)* Which road has the most average traffic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "data_long.groupby('road_name')\\\n",
    "         .value.agg('mean')\\\n",
    "         .reset_index()\\\n",
    "         .sort_values('value', ascending = False)\\\n",
    "         .head(10)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.5.20:** *(Bonus)* Compute annual, average road traffic during day hours (9-17). Which station had the least traffic in 2013? Which station has seen highest growth in traffic from 2013 to 2014?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# create year variable\n",
    "data_long['year'] = data_long.timestamp.dt.year\n",
    "data_long['hour'] = data_long.timestamp.dt.hour\n",
    "\n",
    "# subset relevant years and times \n",
    "data_delta = data_long.query(\"year == 2013 | year == 2014\")\\\n",
    "                      .query(\"hour >= 9 & hour <= 17\")\n",
    "\n",
    "# convert to wide format\n",
    "data_delta = data_delta[['road_name', 'value', 'year']].groupby(['road_name', 'year'])\\\n",
    "                                                       .agg('mean')\\\n",
    "                                                       .reset_index()\\\n",
    "\n",
    "# create year variable\n",
    "data_long['year'] = data_long.timestamp.dt.year\n",
    "data_long['hour'] = data_long.timestamp.dt.hour\n",
    "\n",
    "# subset relevant years and times \n",
    "data_delta = data_long.query(\"year == 2013 | year == 2014\")\\\n",
    "                      .query(\"hour >= 9 & hour <= 17\")\n",
    "\n",
    "# convert to wide format\n",
    "data_delta = data_delta[['road_name', 'value', 'year']].groupby(['road_name', 'year'])\\\n",
    "                                                       .agg('mean')\\\n",
    "                                                       .reset_index()\\\n",
    "                                                       .pivot('road_name', 'year', 'value')\\\n",
    "                                                       .reset_index()\n",
    "# remove redundant column name after pivoting \n",
    "data_delta.columns.name = None\n",
    "\n",
    "# calculate relative size\n",
    "data_delta['growth'] = (data_delta[2014]/data_delta[2013] - 1)*100\n",
    "\n",
    "# show\n",
    "data_delta.sort_values('growth', ascending = False)\n",
    "### END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
